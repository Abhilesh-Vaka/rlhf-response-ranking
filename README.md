# ðŸ§  RLHF (Response Ranking) Dataset

## ðŸ“˜ Overview
This repository contains a **Response Ranking dataset** created manually to simulate the **Reinforcement Learning from Human Feedback (RLHF)** process.  
Each entry has two model responses (A and B) to the same prompt, with a human-ranked preference.

---

## ðŸ“‚ Dataset Structure
| Column | Description |
|--------|-------------|
| **prompt** | The user instruction or query |
| **response_A** | Response generated by the first model |
| **response_B** | Response generated by the second model |
| **preferred** | The preferred response (`A` or `B`) based on helpfulness, clarity, and safety |

---

## ðŸ§° Tools Used
- **ChatGPT (Free Version)**
- **Gemini Free / Hugging Face Models**
- **Google Colab + Pandas** (for dataset creation)
- **GitHub** (for hosting)

---

## ðŸŽ¯ Purpose
This dataset can be used for:
- Training or evaluating RLHF models
- Research on response quality and alignment
- Simulating preference learning tasks

---
